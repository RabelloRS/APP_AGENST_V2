from datetime import datetime
from typing import Any, Dict, List, Optional
import os
import re
import requests
from pathlib import Path
from urllib.parse import urlparse, parse_qs

import numpy as np
import pandas as pd
from thefuzz import fuzz, process
from crewai_tools import WebsiteSearchTool


def read_excel_column(file_path: str, column_name: str) -> list:
    """L√™ uma coluna espec√≠fica de um arquivo Excel."""
    try:
        df = pd.read_excel(file_path)
        if column_name not in df.columns:
            raise ValueError(
                f"Coluna '{column_name}' n√£o encontrada. Colunas dispon√≠veis: {list(df.columns)}"
            )
        return df[column_name].astype(str).tolist()
    except Exception as e:
        raise Exception(f"Erro ao ler arquivo Excel: {str(e)}")


def read_excel_file(file_path: str) -> Dict[str, Any]:
    """L√™ um arquivo Excel completo e retorna informa√ß√µes estruturadas."""
    try:
        df = pd.read_excel(file_path)
        return {
            "columns": df.columns.tolist(),
            "rows": len(df),
            "data_types": df.dtypes.astype(str).to_dict(),
            "sample_data": df.head(5).to_dict("records"),
            "summary_stats": df.describe().to_dict()
            if df.select_dtypes(include=[np.number]).shape[1] > 0
            else {},
        }
    except Exception as e:
        raise Exception(f"Erro ao ler arquivo Excel: {str(e)}")


def compare_text_similarity(list1: list, list2: list) -> dict:
    """Compara similaridade de textos entre duas listas."""
    matches = {}
    for text in list1:
        best_match, score = process.extractOne(
            text, list2, scorer=fuzz.token_sort_ratio
        )
        matches[text] = {"match": best_match, "score": score}
    return matches


def analyze_excel_similarity(
    file1_path: str, file2_path: str, column1: str, column2: str
) -> Dict[str, Any]:
    """An√°lise completa de similaridade entre duas planilhas."""
    try:
        # Ler as colunas
        list1 = read_excel_column(file1_path, column1)
        list2 = read_excel_column(file2_path, column2)

        # Comparar similaridade
        similarity_results = compare_text_similarity(list1, list2)

        # Calcular estat√≠sticas
        scores = [result["score"] for result in similarity_results.values()]

        analysis = {
            "file1_info": {
                "file": file1_path,
                "column": column1,
                "total_items": len(list1),
            },
            "file2_info": {
                "file": file2_path,
                "column": column2,
                "total_items": len(list2),
            },
            "similarity_analysis": {
                "average_score": np.mean(scores),
                "median_score": np.median(scores),
                "max_score": max(scores),
                "min_score": min(scores),
                "high_similarity_count": len([s for s in scores if s >= 80]),
                "medium_similarity_count": len([s for s in scores if 50 <= s < 80]),
                "low_similarity_count": len([s for s in scores if s < 50]),
            },
            "detailed_matches": similarity_results,
            "recommendations": generate_similarity_recommendations(scores),
        }

        return analysis

    except Exception as e:
        raise Exception(f"Erro na an√°lise de similaridade: {str(e)}")


def generate_similarity_recommendations(scores: List[float]) -> List[str]:
    """Gera recomenda√ß√µes baseadas nos scores de similaridade."""
    recommendations = []

    avg_score = np.mean(scores)
    high_similarity = len([s for s in scores if s >= 80])
    total_items = len(scores)

    if avg_score >= 85:
        recommendations.append(
            "‚úÖ Alta similaridade geral - os dados s√£o muito similares"
        )
    elif avg_score >= 70:
        recommendations.append("‚ö†Ô∏è Similaridade moderada - verificar inconsist√™ncias")
    else:
        recommendations.append("‚ùå Baixa similaridade - poss√≠vel problema nos dados")

    if high_similarity / total_items >= 0.8:
        recommendations.append("‚úÖ Mais de 80% dos itens t√™m alta similaridade")
    elif high_similarity / total_items >= 0.5:
        recommendations.append("‚ö†Ô∏è Apenas metade dos itens t√™m alta similaridade")
    else:
        recommendations.append("‚ùå Menos da metade dos itens t√™m alta similaridade")

    return recommendations


def detect_data_patterns(file_path: str, column_name: str) -> Dict[str, Any]:
    """Detecta padr√µes nos dados de uma coluna."""
    try:
        df = pd.read_excel(file_path)
        column_data = df[column_name]

        patterns = {
            "data_type": str(column_data.dtype),
            "unique_values": column_data.nunique(),
            "null_values": column_data.isnull().sum(),
            "duplicates": column_data.duplicated().sum(),
        }

        # Detectar padr√µes espec√≠ficos
        if column_data.dtype == "object":
            # Padr√µes em texto
            text_data = column_data.astype(str)
            patterns["text_patterns"] = {
                "avg_length": text_data.str.len().mean(),
                "max_length": text_data.str.len().max(),
                "min_length": text_data.str.len().min(),
                "common_prefixes": detect_common_prefixes(text_data),
                "common_suffixes": detect_common_suffixes(text_data),
            }
        elif pd.api.types.is_numeric_dtype(column_data):
            # Padr√µes num√©ricos
            numeric_data = column_data.astype(float)
            patterns["numeric_patterns"] = {
                "range": f"{numeric_data.min()} - {numeric_data.max()}",
                "distribution": "normal" if abs(numeric_data.skew()) < 1 else "skewed",
                "outliers": detect_outliers(numeric_data),
            }

        return patterns

    except Exception as e:
        raise Exception(f"Erro ao detectar padr√µes: {str(e)}")


def detect_common_prefixes(data: pd.Series) -> List[str]:
    """Detecta prefixos comuns em dados textuais."""
    prefixes = []
    for i in range(1, 6):  # Verificar prefixos de 1 a 5 caracteres
        prefix_counts = data.str[:i].value_counts()
        common = prefix_counts[prefix_counts >= len(data) * 0.1]  # 10% ou mais
        prefixes.extend(common.index.tolist())
    return list(set(prefixes))[:5]  # Retornar at√© 5 prefixos √∫nicos


def detect_common_suffixes(data: pd.Series) -> List[str]:
    """Detecta sufixos comuns em dados textuais."""
    suffixes = []
    for i in range(1, 6):  # Verificar sufixos de 1 a 5 caracteres
        suffix_counts = data.str[-i:].value_counts()
        common = suffix_counts[suffix_counts >= len(data) * 0.1]  # 10% ou mais
        suffixes.extend(common.index.tolist())
    return list(set(suffixes))[:5]  # Retornar at√© 5 sufixos √∫nicos


def detect_outliers(data: pd.Series) -> Dict[str, Any]:
    """Detecta outliers em dados num√©ricos."""
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data < lower_bound) | (data > upper_bound)]

    return {
        "count": len(outliers),
        "percentage": len(outliers) / len(data) * 100,
        "values": outliers.tolist(),
    }


def generate_excel_report(analysis_results: Dict[str, Any]) -> str:
    """Gera um relat√≥rio estruturado da an√°lise."""
    report = f"""
# Relat√≥rio de An√°lise de Dados

## Resumo Executivo
- **Arquivo 1:** {analysis_results.get('file1_info', {}).get('file', 'N/A')}
- **Arquivo 2:** {analysis_results.get('file2_info', {}).get('file', 'N/A')}
- **Score M√©dio de Similaridade:** {analysis_results.get('similarity_analysis', {}).get('average_score', 0):.2f}%

## An√°lise Detalhada
- **Similaridade Alta (‚â•80%):** {analysis_results.get('similarity_analysis', {}).get('high_similarity_count', 0)} itens
- **Similaridade M√©dia (50-79%):** {analysis_results.get('similarity_analysis', {}).get('medium_similarity_count', 0)} itens
- **Similaridade Baixa (<50%):** {analysis_results.get('similarity_analysis', {}).get('low_similarity_count', 0)} itens

## Recomenda√ß√µes
"""
    for rec in analysis_results.get("recommendations", []):
        report += f"- {rec}\n"

    return report


def validate_excel_file(file_path: str) -> Dict[str, Any]:
    """Valida um arquivo Excel e retorna informa√ß√µes sobre sua estrutura."""
    try:
        df = pd.read_excel(file_path)
        
        validation = {
            "is_valid": True,
            "file_path": file_path,
            "file_size_mb": round(os.path.getsize(file_path) / (1024 * 1024), 2),
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "column_names": df.columns.tolist(),
            "data_types": df.dtypes.astype(str).to_dict(),
            "null_counts": df.isnull().sum().to_dict(),
            "duplicate_rows": df.duplicated().sum(),
            "memory_usage_mb": round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2),
        }
        
        # Verificar problemas comuns
        issues = []
        if df.empty:
            issues.append("Arquivo est√° vazio")
        if df.isnull().sum().sum() > len(df) * 0.5:
            issues.append("Mais de 50% dos dados s√£o nulos")
        if df.duplicated().sum() > len(df) * 0.1:
            issues.append("Mais de 10% das linhas s√£o duplicadas")
            
        validation["issues"] = issues
        validation["has_issues"] = len(issues) > 0
        
        return validation
        
    except Exception as e:
        return {
            "is_valid": False,
            "error": str(e),
            "file_path": file_path
        }


# ============================================================================
# FERRAMENTAS PARA WHATSAPP
# ============================================================================


def whatsapp_connect(session_name: str) -> Dict[str, Any]:
    """Estabelece conex√£o com o WhatsApp Web."""
    try:
        # Implementa√ß√£o simulada para demonstra√ß√£o
        # Em produ√ß√£o, voc√™ precisaria usar selenium ou uma biblioteca espec√≠fica
        
        return {
            "status": "connected",
            "session_name": session_name,
            "message": "WhatsApp Web conectado (simula√ß√£o). Em produ√ß√£o, use selenium."
        }
        
    except Exception as e:
        return {
            "status": "error",
            "session_name": session_name,
            "error": str(e)
        }


def whatsapp_get_messages(group_name: str, limit: int = 100) -> List[Dict[str, Any]]:
    """Obt√©m mensagens de um grupo espec√≠fico do WhatsApp."""
    try:
        # Esta √© uma implementa√ß√£o simulada
        # Em produ√ß√£o, voc√™ precisaria usar uma biblioteca como python-whatsapp-sdk
        # ou selenium para interagir com o WhatsApp Web
        
        messages = []
        # Simular mensagens para demonstra√ß√£o
        for i in range(min(limit, 10)):
            messages.append({
                "id": f"msg_{i}",
                "text": f"Mensagem de exemplo {i}",
                "timestamp": datetime.now().isoformat(),
                "sender": f"Usu√°rio {i % 3}",
                "has_file": i % 3 == 0,
                "file_name": f"arquivo_{i}.pdf" if i % 3 == 0 else None,
                "file_url": f"https://drive.google.com/file/d/example_{i}" if i % 3 == 0 else None
            })
        
        return messages
        
    except Exception as e:
        raise Exception(f"Erro ao obter mensagens: {str(e)}")


def extract_cloud_links(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Extrai links de servi√ßos em nuvem das mensagens."""
    cloud_links = []
    
    # Padr√µes para diferentes servi√ßos de nuvem
    patterns = {
        "google_drive": r"https://drive\.google\.com/file/d/([a-zA-Z0-9_-]+)",
        "google_drive_share": r"https://drive\.google\.com/open\?id=([a-zA-Z0-9_-]+)",
        "onedrive": r"https://1drv\.ms/[a-zA-Z0-9_-]+",
        "dropbox": r"https://www\.dropbox\.com/[a-zA-Z0-9_-]+",
        "mega": r"https://mega\.nz/[a-zA-Z0-9_-]+",
        "mediafire": r"https://www\.mediafire\.com/[a-zA-Z0-9_-]+"
    }
    
    for message in messages:
        text = message.get("text", "")
        
        for service, pattern in patterns.items():
            matches = re.findall(pattern, text)
            for match in matches:
                cloud_links.append({
                    "message_id": message.get("id"),
                    "service": service,
                    "url": match if service == "google_drive" else text,
                    "timestamp": message.get("timestamp"),
                    "sender": message.get("sender")
                })
    
    return cloud_links


def download_cloud_file(cloud_link: str, output_path: str) -> Dict[str, Any]:
    """Baixa arquivo de servi√ßos em nuvem."""
    try:
        # Criar diret√≥rio se n√£o existir
        Path(output_path).mkdir(parents=True, exist_ok=True)
        
        # Extrair nome do arquivo da URL
        parsed_url = urlparse(cloud_link)
        
        if "drive.google.com" in cloud_link:
            # Google Drive
            file_id = parse_qs(parsed_url.query).get('id', [None])[0]
            if not file_id:
                # Tentar extrair do path
                path_parts = parsed_url.path.split('/')
                file_id = path_parts[-1] if path_parts else None
            
            if file_id:
                download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
                response = requests.get(download_url, stream=True)
                
                # Tentar obter nome do arquivo do header
                filename = response.headers.get('content-disposition', '')
                if 'filename=' in filename:
                    filename = filename.split('filename=')[1].strip('"')
                else:
                    filename = f"google_drive_file_{file_id}"
                
                file_path = os.path.join(output_path, filename)
                
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                return {
                    "status": "success",
                    "file_path": file_path,
                    "file_size": os.path.getsize(file_path),
                    "source": "google_drive"
                }
        
        # Para outros servi√ßos, implementar conforme necess√°rio
        return {
            "status": "not_implemented",
            "message": f"Download para {parsed_url.netloc} n√£o implementado ainda"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "cloud_link": cloud_link
        }


def download_whatsapp_file(message: Dict[str, Any], output_path: str) -> Dict[str, Any]:
    """Baixa arquivo anexado a uma mensagem do WhatsApp."""
    try:
        # Criar diret√≥rio se n√£o existir
        Path(output_path).mkdir(parents=True, exist_ok=True)
        
        if not message.get("has_file"):
            return {
                "status": "no_file",
                "message": "Mensagem n√£o cont√©m arquivo"
            }
        
        # Em produ√ß√£o, voc√™ precisaria implementar a l√≥gica real de download
        # do WhatsApp Web usando selenium ou uma biblioteca espec√≠fica
        
        # Simula√ß√£o para demonstra√ß√£o
        filename = message.get("file_name", "arquivo_whatsapp")
        file_path = os.path.join(output_path, filename)
        
        # Criar arquivo de exemplo
        with open(file_path, 'w') as f:
            f.write(f"Arquivo baixado do WhatsApp - {message.get('timestamp')}")
        
        return {
            "status": "success",
            "file_path": file_path,
            "file_size": os.path.getsize(file_path),
            "source": "whatsapp",
            "original_message": message.get("id")
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message_id": message.get("id")
        }


def rename_file_with_timestamp(file_path: str, timestamp: str) -> str:
    """Adiciona timestamp ao nome do arquivo."""
    try:
        # Converter timestamp para formato de data/hora
        if isinstance(timestamp, str):
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        else:
            dt = timestamp
        
        # Formatar data/hora
        timestamp_str = dt.strftime("%Y%m%d_%H%M%S")
        
        # Obter extens√£o do arquivo
        file_ext = Path(file_path).suffix
        file_name = Path(file_path).stem
        
        # Criar novo nome
        new_name = f"{timestamp_str}_{file_name}{file_ext}"
        new_path = Path(file_path).parent / new_name
        
        # Renomear arquivo
        Path(file_path).rename(new_path)
        
        return str(new_path)
        
    except Exception as e:
        raise Exception(f"Erro ao renomear arquivo: {str(e)}")


def organize_files_by_date(files_list: List[Dict[str, Any]], base_path: str) -> Dict[str, Any]:
    """Organiza arquivos em pastas por data de download."""
    try:
        organized_files = {}
        
        for file_info in files_list:
            file_path = file_info.get("file_path")
            timestamp = file_info.get("timestamp")
            
            if not file_path or not timestamp:
                continue

            # Converter timestamp para data
            if isinstance(timestamp, str):
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = timestamp
            
            # Criar pasta por data
            date_folder = dt.strftime("%Y-%m-%d")
            folder_path = Path(base_path) / date_folder
            folder_path.mkdir(parents=True, exist_ok=True)
            
            # Mover arquivo para pasta
            file_name = Path(file_path).name
            new_file_path = folder_path / file_name
            
            if Path(file_path).exists():
                Path(file_path).rename(new_file_path)
                organized_files[file_path] = str(new_file_path)

        return {
            "status": "success",
            "organized_files": organized_files,
            "base_path": base_path
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "files_list": files_list
        }


def simple_research_tool(topic: str) -> str:
    """
    Ferramenta de pesquisa simples que utiliza o WebsiteSearchTool do CrewAI para pesquisar um t√≥pico na web.
    """
    try:
        # Verificar se o t√≥pico √© v√°lido
        if not topic or len(topic.strip()) < 3:
            return f"Erro: T√≥pico de pesquisa muito curto ou vazio. Forne√ßa um t√≥pico com pelo menos 3 caracteres."

        # Criar inst√¢ncia da ferramenta
        try:
            search_tool = WebsiteSearchTool()
        except Exception as tool_error:
            return f"Erro ao inicializar WebsiteSearchTool: {tool_error}"

        # Executar pesquisa
        try:
            print(f"üîç Executando pesquisa web para: {topic}")
            result = search_tool.run(topic)
            if result and len(str(result)) > 20:
                return str(result)
            else:
                return f"Pesquisa para '{topic}' retornou resultado vazio ou muito curto. Tente reformular a pesquisa."
        except Exception as search_error:
            return f"Erro na execu√ß√£o da pesquisa para '{topic}': {search_error}"
    except Exception as e:
        return f"Erro geral na pesquisa para '{topic}': {e}"


# === FERRAMENTAS DE AVALIA√á√ÉO DE CREWS ===

def crew_performance_analyzer(
    execution_data: dict, performance_metrics: Optional[list] = None
) -> str:
    """
    Analisa a performance geral da crew, tempo de execu√ß√£o, qualidade das entregas e colabora√ß√£o entre agentes
    """
    try:
        if performance_metrics is None:
            performance_metrics = ['efficiency', 'quality', 'collaboration', 'completeness']
        
        analysis = {
            'execution_time': execution_data.get('duration', 'N/A'),
            'status': execution_data.get('status', 'unknown'),
            'agents_count': execution_data.get('agents_count', 0),
            'tasks_count': execution_data.get('tasks_count', 0),
            'success_rate': 'N/A'
        }
        
        report = f"""
=== AN√ÅLISE DE PERFORMANCE DA CREW ===

üìä M√âTRICAS GERAIS:
‚Ä¢ Tempo de Execu√ß√£o: {analysis['execution_time']}
‚Ä¢ Status da Execu√ß√£o: {analysis['status']}
‚Ä¢ N√∫mero de Agentes: {analysis['agents_count']}
‚Ä¢ N√∫mero de Tarefas: {analysis['tasks_count']}

üìà AN√ÅLISE DE PERFORMANCE:
‚Ä¢ Efici√™ncia Temporal: {'Boa' if 'completed' in analysis['status'] else 'Necessita Melhoria'}
‚Ä¢ Qualidade da Entrega: {'Analisando outputs individuais...'}
‚Ä¢ Colabora√ß√£o entre Agentes: {'Sem conflitos detectados' if analysis['agents_count'] > 1 else 'Agente √∫nico'}

üéØ INDICADORES-CHAVE:
‚Ä¢ Completude das Tarefas: {'100%' if analysis['status'] == 'completed' else 'Incompleto'}
‚Ä¢ Uso de Recursos: {'Otimizado' if analysis['execution_time'] else 'A analisar'}
        """
        
        return report.strip()
        
    except Exception as e:
        return f"Erro na an√°lise de performance da crew: {str(e)}"

def agent_output_quality_checker(
    agent_outputs: dict, quality_criteria: Optional[list] = None
) -> str:
    """
    Avalia a qualidade, completude e relev√¢ncia dos outputs produzidos por cada agente individualmente
    """
    try:
        if quality_criteria is None:
            quality_criteria = ['relevance', 'completeness', 'accuracy', 'clarity']
        
        analysis_report = "=== AN√ÅLISE DE QUALIDADE DOS OUTPUTS DOS AGENTES ===\n\n"
        
        for agent_name, output in agent_outputs.items():
            output_length = len(str(output)) if output else 0
            
            quality_score = min(100, max(10, output_length / 10))  # Score b√°sico baseado no tamanho
            
            analysis_report += f"""
ü§ñ AGENTE: {agent_name}
‚Ä¢ Tamanho do Output: {output_length} caracteres
‚Ä¢ Score de Qualidade: {quality_score:.1f}/100
‚Ä¢ Completude: {'Boa' if output_length > 100 else 'Insuficiente'}
‚Ä¢ Relev√¢ncia: {'A verificar conte√∫do espec√≠fico'}
‚Ä¢ Clareza: {'Boa' if output_length > 50 else 'Limitada'}

"""
        
        return analysis_report
        
    except Exception as e:
        return f"Erro na verifica√ß√£o de qualidade dos outputs: {str(e)}"

def tool_usage_evaluator(
    tools_used: dict, task_requirements: Optional[dict] = None
) -> str:
    """
    Analisa se as ferramentas foram utilizadas adequadamente por cada agente
    """
    try:
        evaluation_report = "=== AVALIA√á√ÉO DO USO DE FERRAMENTAS ===\n\n"
        
        for agent_name, agent_tools in tools_used.items():
            evaluation_report += f"""
üîß AGENTE: {agent_name}
‚Ä¢ Ferramentas Dispon√≠veis: {len(agent_tools) if agent_tools else 0}
‚Ä¢ Ferramentas Listadas: {', '.join(agent_tools) if agent_tools else 'Nenhuma'}
‚Ä¢ Adequa√ß√£o: {'Adequado' if agent_tools else 'Sem ferramentas - PROBLEMA!'}
‚Ä¢ Sugest√µes: {'Verificar se as ferramentas s√£o espec√≠ficas para as tarefas executadas'}

"""
        
        # Recomenda√ß√µes gerais
        evaluation_report += """
üéØ RECOMENDA√á√ïES GERAIS:
‚Ä¢ Verificar se cada agente tem as ferramentas adequadas para sua especializa√ß√£o
‚Ä¢ Considerar adicionar ferramentas de valida√ß√£o e verifica√ß√£o
‚Ä¢ Avaliar se existem ferramentas subutilizadas que poderiam melhorar a performance
‚Ä¢ Implementar ferramentas de colabora√ß√£o entre agentes quando necess√°rio
        """
        
        return evaluation_report
        
    except Exception as e:
        return f"Erro na avalia√ß√£o do uso de ferramentas: {str(e)}"

def workflow_efficiency_analyzer(
    workflow_data: dict, execution_timeline: Optional[list] = None
) -> str:
    """
    Avalia a efici√™ncia do fluxo de trabalho entre agentes, identificando gargalos e pontos de melhoria
    """
    try:
        analysis_report = "=== AN√ÅLISE DE EFICI√äNCIA DO FLUXO DE TRABALHO ===\n\n"
        
        # An√°lise b√°sica do fluxo
        agents_count = workflow_data.get('agents_count', 0)
        tasks_count = workflow_data.get('tasks_count', 0)
        duration = workflow_data.get('duration', 'N/A')
        
        analysis_report += f"""
üìã VIS√ÉO GERAL DO FLUXO:
‚Ä¢ Agentes Envolvidos: {agents_count}
‚Ä¢ Tarefas Executadas: {tasks_count}
‚Ä¢ Dura√ß√£o Total: {duration}
‚Ä¢ Paraleliza√ß√£o: {'Poss√≠vel' if agents_count > 1 else 'Limitada (agente √∫nico)'}

üîç AN√ÅLISE DE EFICI√äNCIA:
‚Ä¢ Sequenciamento: {'Sequencial' if tasks_count > 1 else 'Tarefa √∫nica'}
‚Ä¢ Gargalos Identificados: {'Analisando depend√™ncias...'}
‚Ä¢ Oportunidades de Otimiza√ß√£o: {'Verificar possibilidade de execu√ß√£o paralela'}

üí° SUGEST√ïES DE MELHORIA:
‚Ä¢ Considerar paraleliza√ß√£o de tarefas independentes
‚Ä¢ Verificar se a ordem das tarefas est√° otimizada
‚Ä¢ Avaliar se h√° agentes subutilizados
‚Ä¢ Implementar sistema de feedback entre agentes
        """
        
        return analysis_report
        
    except Exception as e:
        return f"Erro na an√°lise de efici√™ncia do fluxo: {str(e)}"

def recommendation_generator(
    analysis_results: dict, improvement_areas: Optional[list] = None
) -> str:
    """
    Gera recomenda√ß√µes espec√≠ficas e acion√°veis para melhorar agentes, ferramentas e tarefas
    """
    try:
        if improvement_areas is None:
            improvement_areas = ['agents', 'tools', 'tasks', 'workflow']
        
        recommendations = "=== RECOMENDA√á√ïES DE MELHORIA ===\n\n"
        
        # Recomenda√ß√µes para Agentes
        recommendations += """
ü§ñ MELHORIAS NOS AGENTES:
1. CONFIGURA√á√ÉO:
   ‚Ä¢ Revisar backstory para ser mais espec√≠fico e detalhado
   ‚Ä¢ Ajustar goals para serem mais precisos e mensur√°veis
   ‚Ä¢ Verificar se o role est√° alinhado com as tarefas executadas

2. ESPECIALIZA√á√ÉO:
   ‚Ä¢ Considerar criar agentes mais especializados para tarefas espec√≠ficas
   ‚Ä¢ Avaliar se h√° sobreposi√ß√£o de responsabilidades entre agentes
   ‚Ä¢ Verificar se os agentes t√™m conhecimento suficiente para suas tarefas

"""
        
        # Recomenda√ß√µes para Ferramentas
        recommendations += """
üîß OTIMIZA√á√ÉO DE FERRAMENTAS:
1. ADEQUA√á√ÉO:
   ‚Ä¢ Verificar se cada agente tem todas as ferramentas necess√°rias
   ‚Ä¢ Considerar adicionar ferramentas de valida√ß√£o espec√≠ficas
   ‚Ä¢ Implementar ferramentas de comunica√ß√£o entre agentes

2. EFICI√äNCIA:
   ‚Ä¢ Avaliar ferramentas que podem acelerar o processo
   ‚Ä¢ Considerar ferramentas de automa√ß√£o para tarefas repetitivas
   ‚Ä¢ Implementar ferramentas de controle de qualidade

"""
        
        # Recomenda√ß√µes para Tarefas
        recommendations += """
üìã REFINAMENTO DE TAREFAS:
1. CLAREZA:
   ‚Ä¢ Tornar descri√ß√µes mais espec√≠ficas e detalhadas
   ‚Ä¢ Definir outputs esperados de forma mais precisa
   ‚Ä¢ Incluir crit√©rios de qualidade nas descri√ß√µes

2. ESTRUTURA:
   ‚Ä¢ Considerar dividir tarefas complexas em subtarefas
   ‚Ä¢ Verificar depend√™ncias entre tarefas
   ‚Ä¢ Implementar sistema de valida√ß√£o de entregas

"""
        
        # Prioriza√ß√£o
        recommendations += """
üéØ PRIORIZA√á√ÉO DAS MELHORIAS:
1. ALTA PRIORIDADE:
   ‚Ä¢ Corrigir agentes sem ferramentas adequadas
   ‚Ä¢ Melhorar descri√ß√µes de tarefas pouco claras
   ‚Ä¢ Implementar valida√ß√µes de qualidade

2. M√âDIA PRIORIDADE:
   ‚Ä¢ Otimizar fluxo de trabalho
   ‚Ä¢ Adicionar ferramentas de efici√™ncia
   ‚Ä¢ Refinar backstories dos agentes

3. BAIXA PRIORIDADE:
   ‚Ä¢ Implementar funcionalidades avan√ßadas
   ‚Ä¢ Otimiza√ß√µes de performance menores
   ‚Ä¢ Melhorias cosm√©ticas na documenta√ß√£o
        """
        
        return recommendations
        
    except Exception as e:
        return f"Erro na gera√ß√£o de recomenda√ß√µes: {str(e)}"

def execution_summary_builder(crew_data: dict, execution_results: dict) -> str:
    """
    Constr√≥i um resumo estruturado e detalhado da execu√ß√£o da crew para an√°lise
    """
    try:
        summary = "=== RESUMO ESTRUTURADO DA EXECU√á√ÉO ===\n\n"
        
        # Informa√ß√µes b√°sicas
        summary += f"""
üìä INFORMA√á√ïES B√ÅSICAS:
‚Ä¢ Crew: {crew_data.get('name', 'N/A')}
‚Ä¢ T√≥pico: {execution_results.get('topic', 'N/A')}
‚Ä¢ Status: {execution_results.get('status', 'N/A')}
‚Ä¢ Dura√ß√£o: {execution_results.get('duration', 'N/A')}
‚Ä¢ Timestamp: {execution_results.get('timestamp', 'N/A')}

"""
        
        # An√°lise dos agentes
        agents = crew_data.get('agents', [])
        summary += f"üë• AGENTES ENVOLVIDOS ({len(agents)}):\n"
        for i, agent in enumerate(agents, 1):
            agent_role = getattr(agent, 'role', 'N/A') if hasattr(agent, 'role') else 'N/A'
            agent_tools = getattr(agent, 'tools', []) if hasattr(agent, 'tools') else []
            summary += f"   {i}. {agent_role} - {len(agent_tools)} ferramentas\n"
        
        summary += "\n"
        
        # An√°lise das tarefas
        tasks = crew_data.get('tasks', [])
        summary += f"üìã TAREFAS EXECUTADAS ({len(tasks)}):\n"
        for i, task in enumerate(tasks, 1):
            task_desc = getattr(task, 'description', 'N/A') if hasattr(task, 'description') else 'N/A'
            summary += f"   {i}. {task_desc[:100]}{'...' if len(task_desc) > 100 else ''}\n"
        
        summary += "\n"
        
        # Resultado final
        result = execution_results.get('result', '')
        result_length = len(str(result))
        summary += f"""
üìÑ RESULTADO FINAL:
‚Ä¢ Tamanho do Output: {result_length} caracteres
‚Ä¢ Qualidade Aparente: {'Boa' if result_length > 500 else 'Limitada' if result_length > 100 else 'Insuficiente'}
‚Ä¢ Completude: {'Completo' if result_length > 200 else 'Incompleto'}
"""
        
        return summary
        
    except Exception as e:
        return f"Erro na constru√ß√£o do resumo de execu√ß√£o: {str(e)}"
